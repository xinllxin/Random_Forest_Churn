
## ML Project 1
 Xinxin Liu
 11/02/2020


# Load Libraries 
```{r, message= FALSE}
library(readr)
library(tidyverse)
library(tidymodels)
library(janitor)
library(skimr)
library(vip)
library(parallel)
library(doParallel)
library(ggpubr)
library(kableExtra)
```


# 1. Import Data 
```{r , message= FALSE, warning= FALSE}
churn_train <- read_csv("Churn_training.csv") %>% clean_names() 
head(churn_train)

# remove & mutate variables

churn_train <- churn_train %>% 
   select (-customer_id,-billing_address,-billing_city,-billing_postal,-ip_address_asn,-phone_area_code, -customer_reg_date, -email_domain, -customer_service_calls,-billing_state, -device_protection, -contract_code, -maling_code, -phone_model) %>% 
  mutate_if(is.character,as.factor) %>%
  mutate(churn = as.factor(churn),
         senior_citizen = as.factor(senior_citizen))

```


## 2. Analyze Target


```{r , message= FALSE, warning= FALSE}
target_summary <- churn_train %>% 
  group_by(churn) %>%
  summarise(n=n()) %>%
  mutate(pct = n/sum(n)) 

text <- ggtexttable(target_summary, rows = NULL, theme = ttheme("mOrange"))

graph1 <- target_summary %>%
  ggplot(aes(x= churn, y = n)) + 
  geom_col() + 
  labs(title = "Churn Count")

graph2 <- target_summary %>%
  ggplot(aes(x= churn, y = pct)) + 
  geom_col() + 
  labs(title = "Churn Rate")

ggarrange(graph2,text, 
          ncol = 1, nrow = 2,
          heights = c(1,0.3))


```
## 3. Explore data


```{r, message= FALSE, warning= FALSE}

histogram <- function(col) {
  plt <- ggplot(churn_train,aes(!!as.name(col), fill = churn)) + 
    geom_histogram() 
  print(plt)
}

i <- 1
for (col in colnames(churn_train %>% select_if(is.numeric))){
  histogram(col)
  i <- i +1
}



histogram <- function(col) {
  plt <- ggplot(churn_train,aes(!!as.name(col), fill = churn)) + 
    geom_histogram(position = "fill") 
  print(plt)
}

i <- 1
for (col in colnames(churn_train %>% select_if(is.numeric))){
  histogram(col)
  i <- i +1
}



```

```{r, message= FALSE}

  target_summary <- churn_train %>% 
  filter(churn == 1) %>%
  group_by(multiple_lines) %>%
  summarise(n=n()) %>%
  mutate(pct = n/sum(n)) 

target_summary

```



```{r, message= FALSE, warning= FALSE}


bar <- function(col) {
  plt <- ggplot(churn_train,aes(!!as.name(col), fill = churn)) +
    geom_bar(position = "fill")
  print(plt)
}

make_table <- function(col) {
  
  
  target_summary <- churn_train %>% 
  filter(churn == 1) %>%
  group_by(!!as.name(col)) %>%
  summarise(n=n()) %>%
  mutate(pct = n/sum(n)) 


text <- ggtexttable(target_summary, rows = NULL, theme = ttheme("mOrange"))


graph2 <- ggplot(churn_train,aes(!!as.name(col), fill = churn)) +
    geom_bar(position = "fill")


plt <- ggarrange(graph2,text, 
          ncol = 1, nrow = 2,
          heights = c(1,0.3))
  print(plt)
}







```



##Correlation
```{r, message= FALSE}
library(corrplot)
library(reshape2)

churn_train %>%
  select_if(is.numeric) %>%
  na.omit() %>%
  cor() %>%
  melt() %>%
  mutate(value = round(value,2)) %>%
  ggplot(aes(x=Var1, y=Var2, fill=value), ) + 
  geom_tile() +
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
   midpoint = 0, limit = c(-1,1), space = "Lab", 
    name="Pearson\nCorrelation") +
  geom_text(aes(Var2, Var1, label = value), color = "black", size = 4)+ 
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Correlation Analysis")
```




## Partrition our data

```{r, message= FALSE}
set.seed(43)

split <- initial_split (churn_train, prop = 0.7)

train <- training(split)
test <- testing(split)

sprintf("Train PCT: %1.2f%%", nrow(train)/nrow(churn_train) * 100)
sprintf("Test PCT : %1.2f%%", nrow(test)/ nrow(churn_train) * 100)

```

## RandomForest Package 

# Recepie, build model, build workflow
```{r, message= FALSE}
rf_recipe <- 
  recipe (churn ~ ., data = train) %>%
  step_medianimpute(all_numeric(), -all_outcomes()) %>%
  step_unknown(all_nominal(), -all_outcomes()) %>%
  step_dummy(all_nominal(), -all_outcomes()) 


  
rf_model <- rand_forest(
  mtry = 6,
  trees = 100,
  min_n = 35) %>%
  set_engine("ranger",
             importance = "permutation",
             max.depth = 10) %>% 
  set_mode("classification")


rf_model2 <- rand_forest(
  mtry = 6,
  trees = 500,
  min_n = 40) %>%
  set_engine("ranger",
             importance = "permutation",
             max.depth = 11) %>% 
  set_mode("classification")



doParallel::registerDoParallel()

rf_workflow <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_model) 

rf_fit <- rf_workflow %>%
  fit(data = train)

rf_workflow2 <- workflow() %>%
  add_recipe(rf_recipe) %>%
  add_model(rf_model2)

rf_fit2 <- rf_workflow2 %>%
    fit(data = train)
  




```


# Make Prediction, Evaluation 

```{r, message= FALSE }
options(yardstick.event_first = FALSE)
 # -- score training  
  predict(rf_fit, train, type="prob") %>%
    bind_cols(predict(rf_fit, train, type="class")) %>%
    bind_cols(.,train)-> scored_train 

  # -- score testing 
  predict(rf_fit, test, type="prob") %>%
      bind_cols(predict(rf_fit,  test, type="class")) %>%
       bind_cols(., test) -> scored_test   

  # -- Metrics: Train and Test 
  scored_train %>% 
    metrics(churn, `.pred_1`, estimate = .pred_class) %>%
    mutate(part="training") %>%
    bind_rows( scored_test %>% 
                 metrics(churn, `.pred_1`, estimate = .pred_class) %>%
                 mutate(part="testing") ) %>%
    filter(.metric %in% c('accuracy','roc_auc')) %>%
    pivot_wider(names_from = .metric, values_from=.estimate) %>% kable() 
```

```{r, message= FALSE}
options(yardstick.event_first = FALSE)
 # -- score training  
  predict(rf_fit2, train, type="prob") %>%
    bind_cols(predict(rf_fit2, train, type="class")) %>%
    bind_cols(.,train)-> scored_train2

  # -- score testing 
  predict(rf_fit2, test, type="prob") %>%
      bind_cols(predict(rf_fit2,  test, type="class")) %>%
       bind_cols(., test) -> scored_test2   

  # -- Metrics: Train and Test 
  scored_train2 %>% 
    metrics(churn, `.pred_1`, estimate = .pred_class) %>%
    mutate(part="training") %>%
    bind_rows( scored_test %>% 
                 metrics(churn, `.pred_1`, estimate = .pred_class) %>%
                 mutate(part="testing") ) %>%
    filter(.metric %in% c('accuracy','roc_auc')) %>%
    pivot_wider(names_from = .metric, values_from=.estimate) %>% kable() 
```



```{r, message= FALSE}

options(yardstick.event_first = FALSE) 

predict(rf_fit, train, type="prob") %>%
  bind_cols(predict(rf_fit, train, type="class")) %>%
  bind_cols(.,train)-> scored_train

predict(rf_fit,test, type = "prob") %>%
  bind_cols(predict(rf_fit, test, type = "class")) %>%
  bind_cols (., test) -> scored_test

scored_train %>% 
  metrics(churn, `.pred_1`, estimate = .pred_class) %>%
  mutate(part="training") %>%
  bind_rows( scored_test %>% 
               metrics(churn, `.pred_1`, estimate = .pred_class) %>%
               mutate(part="testing") ) %>%
  filter(.metric %in% c('accuracy','roc_auc')) %>%
  pivot_wider(names_from = .metric, values_from=.estimate) 




predict(rf_fit2, train, type="prob") %>%
  bind_cols(predict(rf_fit2, train, type="class")) %>%
  bind_cols(.,train)-> scored_train2


predict(rf_fit2,test, type = "prob") %>%
  bind_cols(predict(rf_fit2, test, type = "class")) %>%
  bind_cols (., test) -> scored_test2


scored_train2 %>% 
  metrics(churn, `.pred_1`, estimate = .pred_class) %>%
  mutate(part="training") %>%
  bind_rows( scored_test2 %>% 
               metrics(churn, `.pred_1`, estimate = .pred_class) %>%
               mutate(part="testing") ) %>%
  filter(.metric %in% c('accuracy','roc_auc')) %>%
  pivot_wider(names_from = .metric, values_from=.estimate)


```

```{r, message= FALSE}

## Random Forest Model 1

  rf_fit %>%
    pull_workflow_fit() %>%
  vip(num_features = 10) + labs(title = "Random Forest 1")
  
  # -- confusion matrix 
  scored_train %>%
    conf_mat(churn, .pred_class) %>%
    autoplot(type = "heatmap")+ labs(title = "Random Forest 1, Train")
  
   scored_test %>%
    conf_mat(churn, .pred_class) %>%
    autoplot(type = "heatmap")+ labs(title = "Random Forest 1, Test")
  
  # -- ROC Charts 
  scored_train %>%
  mutate(model = "train") %>%
  bind_rows(scored_test %>%
              mutate(model="test")) %>%
  group_by(model) %>%
  roc_curve(churn, `.pred_1`) %>%
  autoplot() + labs(title = "Random Forest 1 ")

  # -- operating range -- 
  scored_test  %>%
  roc_curve(churn, `.pred_1`) %>%
  mutate(fpr = round((1 - specificity),2),
         tpr = round(sensitivity,3),
         score_threshold =  1- round(.threshold,3)) %>%
  group_by(fpr) %>%
  summarise(score_threshold = max(score_threshold),
            tpr = max(tpr))%>%
  ungroup() %>%
  mutate(precision = tpr/(tpr + fpr)) %>%
  select(fpr, tpr, precision, score_threshold) %>%
  filter(fpr <= 0.1) %>% kable()

  
## Random Forest Model 2

  # -- variable importance: top 10
  rf_fit2 %>%
    pull_workflow_fit() %>%
  vip(num_features = 10)+ labs(title = "Random Forest 2")
  
  # -- confusion matrix 
  scored_train2 %>%
    conf_mat(churn, .pred_class) %>%
    autoplot(type = "heatmap") + labs(title = "Random Forest 2, Train")
  
   scored_test2 %>%
    conf_mat(churn, .pred_class) %>%
    autoplot(type = "heatmap") + labs(title = "Random Forest 2, Test")
  
  # -- ROC Charts 
  scored_train2 %>%
  mutate(model = "train") %>%
  bind_rows(scored_test %>%
              mutate(model="test")) %>%
  group_by(model) %>%
  roc_curve(churn, `.pred_1`) %>%
  autoplot()  + 
  labs(title = "Random Forest 2")

  # -- operating range -- 
  scored_test2  %>%
  roc_curve(churn, `.pred_1`) %>%
  mutate(fpr = round((1 - specificity),2),
         tpr = round(sensitivity,3),
         score_threshold =  1- round(.threshold,3)) %>%
  group_by(fpr) %>%
  summarise(score_threshold = max(score_threshold),
            tpr = max(tpr))%>%
  ungroup() %>%
  mutate(precision = tpr/(tpr + fpr)) %>%
  select(fpr, tpr, precision, score_threshold) %>%
  filter(fpr <= 0.1) %>% kable() 

```

## Predictied Test Score Distribution

```{r, message= FALSE}
scored_test %>% ggplot(aes(.pred_1)) + geom_histogram() + labs(title = "Random Forest 1, Test, predictied socre distribution")

scored_test2 %>% ggplot(aes(.pred_1)) + geom_histogram() + labs(title = "Random Forest 2, Test, predictied socre distribution")
```


## Make Prediction to Holdout File 

```{r,eval=FALSE}

churn_holdout <- read_csv("Churn_holdout.csv") %>% clean_names() 

# remove & mutate variables

churn_holdout_clean <- churn_holdout %>% 
  select (-customer_id,-billing_address,-billing_city,-billing_postal,-ip_address_asn,-phone_area_code, -customer_reg_date, -email_domain, -customer_service_calls,phone_model,-billing_state, -device_protection, -contract_code,maling_code) %>% 
  mutate_if(is.character,as.factor) %>%
  mutate(senior_citizen = as.factor(senior_citizen)) 

predict(rf_workflow2,churn_holdout_clean, type = "prob") %>%
  bind_cols(., churn_holdout %>% select(customer_id)) %>%
  select(customer_id, churn = .pred_1) ->scored_holdout
  
  



```
# check score distribution from predictied holdout file 
```{r, message= FALSE}


scored_holdout <- read_csv("scored_holdout.csv")
head(scored_holdout)


```
```{r, message= FALSE}
scored_holdout %>% ggplot(aes(churn)) + geom_histogram() + labs(title = "Holdout predictied socre distribution")
```

